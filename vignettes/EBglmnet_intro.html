<!DOCTYPE html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>

<title>output: pdf_document</title>

<script type="text/javascript">
window.onload = function() {
  var imgs = document.getElementsByTagName('img'), i, img;
  for (i = 0; i < imgs.length; i++) {
    img = imgs[i];
    // center an image if it is the only element of its parent
    if (img.parentElement.childElementCount === 1)
      img.parentElement.style.textAlign = 'center';
  }
};
</script>

<!-- Styles for R syntax highlighter -->
<style type="text/css">
   pre .operator,
   pre .paren {
     color: rgb(104, 118, 135)
   }

   pre .literal {
     color: #990073
   }

   pre .number {
     color: #099;
   }

   pre .comment {
     color: #998;
     font-style: italic
   }

   pre .keyword {
     color: #900;
     font-weight: bold
   }

   pre .identifier {
     color: rgb(0, 0, 0);
   }

   pre .string {
     color: #d14;
   }
</style>

<!-- R syntax highlighter -->
<script type="text/javascript">
var hljs=new function(){function m(p){return p.replace(/&/gm,"&amp;").replace(/</gm,"&lt;")}function f(r,q,p){return RegExp(q,"m"+(r.cI?"i":"")+(p?"g":""))}function b(r){for(var p=0;p<r.childNodes.length;p++){var q=r.childNodes[p];if(q.nodeName=="CODE"){return q}if(!(q.nodeType==3&&q.nodeValue.match(/\s+/))){break}}}function h(t,s){var p="";for(var r=0;r<t.childNodes.length;r++){if(t.childNodes[r].nodeType==3){var q=t.childNodes[r].nodeValue;if(s){q=q.replace(/\n/g,"")}p+=q}else{if(t.childNodes[r].nodeName=="BR"){p+="\n"}else{p+=h(t.childNodes[r])}}}if(/MSIE [678]/.test(navigator.userAgent)){p=p.replace(/\r/g,"\n")}return p}function a(s){var r=s.className.split(/\s+/);r=r.concat(s.parentNode.className.split(/\s+/));for(var q=0;q<r.length;q++){var p=r[q].replace(/^language-/,"");if(e[p]){return p}}}function c(q){var p=[];(function(s,t){for(var r=0;r<s.childNodes.length;r++){if(s.childNodes[r].nodeType==3){t+=s.childNodes[r].nodeValue.length}else{if(s.childNodes[r].nodeName=="BR"){t+=1}else{if(s.childNodes[r].nodeType==1){p.push({event:"start",offset:t,node:s.childNodes[r]});t=arguments.callee(s.childNodes[r],t);p.push({event:"stop",offset:t,node:s.childNodes[r]})}}}}return t})(q,0);return p}function k(y,w,x){var q=0;var z="";var s=[];function u(){if(y.length&&w.length){if(y[0].offset!=w[0].offset){return(y[0].offset<w[0].offset)?y:w}else{return w[0].event=="start"?y:w}}else{return y.length?y:w}}function t(D){var A="<"+D.nodeName.toLowerCase();for(var B=0;B<D.attributes.length;B++){var C=D.attributes[B];A+=" "+C.nodeName.toLowerCase();if(C.value!==undefined&&C.value!==false&&C.value!==null){A+='="'+m(C.value)+'"'}}return A+">"}while(y.length||w.length){var v=u().splice(0,1)[0];z+=m(x.substr(q,v.offset-q));q=v.offset;if(v.event=="start"){z+=t(v.node);s.push(v.node)}else{if(v.event=="stop"){var p,r=s.length;do{r--;p=s[r];z+=("</"+p.nodeName.toLowerCase()+">")}while(p!=v.node);s.splice(r,1);while(r<s.length){z+=t(s[r]);r++}}}}return z+m(x.substr(q))}function j(){function q(x,y,v){if(x.compiled){return}var u;var s=[];if(x.k){x.lR=f(y,x.l||hljs.IR,true);for(var w in x.k){if(!x.k.hasOwnProperty(w)){continue}if(x.k[w] instanceof Object){u=x.k[w]}else{u=x.k;w="keyword"}for(var r in u){if(!u.hasOwnProperty(r)){continue}x.k[r]=[w,u[r]];s.push(r)}}}if(!v){if(x.bWK){x.b="\\b("+s.join("|")+")\\s"}x.bR=f(y,x.b?x.b:"\\B|\\b");if(!x.e&&!x.eW){x.e="\\B|\\b"}if(x.e){x.eR=f(y,x.e)}}if(x.i){x.iR=f(y,x.i)}if(x.r===undefined){x.r=1}if(!x.c){x.c=[]}x.compiled=true;for(var t=0;t<x.c.length;t++){if(x.c[t]=="self"){x.c[t]=x}q(x.c[t],y,false)}if(x.starts){q(x.starts,y,false)}}for(var p in e){if(!e.hasOwnProperty(p)){continue}q(e[p].dM,e[p],true)}}function d(B,C){if(!j.called){j();j.called=true}function q(r,M){for(var L=0;L<M.c.length;L++){if((M.c[L].bR.exec(r)||[null])[0]==r){return M.c[L]}}}function v(L,r){if(D[L].e&&D[L].eR.test(r)){return 1}if(D[L].eW){var M=v(L-1,r);return M?M+1:0}return 0}function w(r,L){return L.i&&L.iR.test(r)}function K(N,O){var M=[];for(var L=0;L<N.c.length;L++){M.push(N.c[L].b)}var r=D.length-1;do{if(D[r].e){M.push(D[r].e)}r--}while(D[r+1].eW);if(N.i){M.push(N.i)}return f(O,M.join("|"),true)}function p(M,L){var N=D[D.length-1];if(!N.t){N.t=K(N,E)}N.t.lastIndex=L;var r=N.t.exec(M);return r?[M.substr(L,r.index-L),r[0],false]:[M.substr(L),"",true]}function z(N,r){var L=E.cI?r[0].toLowerCase():r[0];var M=N.k[L];if(M&&M instanceof Array){return M}return false}function F(L,P){L=m(L);if(!P.k){return L}var r="";var O=0;P.lR.lastIndex=0;var M=P.lR.exec(L);while(M){r+=L.substr(O,M.index-O);var N=z(P,M);if(N){x+=N[1];r+='<span class="'+N[0]+'">'+M[0]+"</span>"}else{r+=M[0]}O=P.lR.lastIndex;M=P.lR.exec(L)}return r+L.substr(O,L.length-O)}function J(L,M){if(M.sL&&e[M.sL]){var r=d(M.sL,L);x+=r.keyword_count;return r.value}else{return F(L,M)}}function I(M,r){var L=M.cN?'<span class="'+M.cN+'">':"";if(M.rB){y+=L;M.buffer=""}else{if(M.eB){y+=m(r)+L;M.buffer=""}else{y+=L;M.buffer=r}}D.push(M);A+=M.r}function G(N,M,Q){var R=D[D.length-1];if(Q){y+=J(R.buffer+N,R);return false}var P=q(M,R);if(P){y+=J(R.buffer+N,R);I(P,M);return P.rB}var L=v(D.length-1,M);if(L){var O=R.cN?"</span>":"";if(R.rE){y+=J(R.buffer+N,R)+O}else{if(R.eE){y+=J(R.buffer+N,R)+O+m(M)}else{y+=J(R.buffer+N+M,R)+O}}while(L>1){O=D[D.length-2].cN?"</span>":"";y+=O;L--;D.length--}var r=D[D.length-1];D.length--;D[D.length-1].buffer="";if(r.starts){I(r.starts,"")}return R.rE}if(w(M,R)){throw"Illegal"}}var E=e[B];var D=[E.dM];var A=0;var x=0;var y="";try{var s,u=0;E.dM.buffer="";do{s=p(C,u);var t=G(s[0],s[1],s[2]);u+=s[0].length;if(!t){u+=s[1].length}}while(!s[2]);if(D.length>1){throw"Illegal"}return{r:A,keyword_count:x,value:y}}catch(H){if(H=="Illegal"){return{r:0,keyword_count:0,value:m(C)}}else{throw H}}}function g(t){var p={keyword_count:0,r:0,value:m(t)};var r=p;for(var q in e){if(!e.hasOwnProperty(q)){continue}var s=d(q,t);s.language=q;if(s.keyword_count+s.r>r.keyword_count+r.r){r=s}if(s.keyword_count+s.r>p.keyword_count+p.r){r=p;p=s}}if(r.language){p.second_best=r}return p}function i(r,q,p){if(q){r=r.replace(/^((<[^>]+>|\t)+)/gm,function(t,w,v,u){return w.replace(/\t/g,q)})}if(p){r=r.replace(/\n/g,"<br>")}return r}function n(t,w,r){var x=h(t,r);var v=a(t);var y,s;if(v){y=d(v,x)}else{return}var q=c(t);if(q.length){s=document.createElement("pre");s.innerHTML=y.value;y.value=k(q,c(s),x)}y.value=i(y.value,w,r);var u=t.className;if(!u.match("(\\s|^)(language-)?"+v+"(\\s|$)")){u=u?(u+" "+v):v}if(/MSIE [678]/.test(navigator.userAgent)&&t.tagName=="CODE"&&t.parentNode.tagName=="PRE"){s=t.parentNode;var p=document.createElement("div");p.innerHTML="<pre><code>"+y.value+"</code></pre>";t=p.firstChild.firstChild;p.firstChild.cN=s.cN;s.parentNode.replaceChild(p.firstChild,s)}else{t.innerHTML=y.value}t.className=u;t.result={language:v,kw:y.keyword_count,re:y.r};if(y.second_best){t.second_best={language:y.second_best.language,kw:y.second_best.keyword_count,re:y.second_best.r}}}function o(){if(o.called){return}o.called=true;var r=document.getElementsByTagName("pre");for(var p=0;p<r.length;p++){var q=b(r[p]);if(q){n(q,hljs.tabReplace)}}}function l(){if(window.addEventListener){window.addEventListener("DOMContentLoaded",o,false);window.addEventListener("load",o,false)}else{if(window.attachEvent){window.attachEvent("onload",o)}else{window.onload=o}}}var e={};this.LANGUAGES=e;this.highlight=d;this.highlightAuto=g;this.fixMarkup=i;this.highlightBlock=n;this.initHighlighting=o;this.initHighlightingOnLoad=l;this.IR="[a-zA-Z][a-zA-Z0-9_]*";this.UIR="[a-zA-Z_][a-zA-Z0-9_]*";this.NR="\\b\\d+(\\.\\d+)?";this.CNR="\\b(0[xX][a-fA-F0-9]+|(\\d+(\\.\\d*)?|\\.\\d+)([eE][-+]?\\d+)?)";this.BNR="\\b(0b[01]+)";this.RSR="!|!=|!==|%|%=|&|&&|&=|\\*|\\*=|\\+|\\+=|,|\\.|-|-=|/|/=|:|;|<|<<|<<=|<=|=|==|===|>|>=|>>|>>=|>>>|>>>=|\\?|\\[|\\{|\\(|\\^|\\^=|\\||\\|=|\\|\\||~";this.ER="(?![\\s\\S])";this.BE={b:"\\\\.",r:0};this.ASM={cN:"string",b:"'",e:"'",i:"\\n",c:[this.BE],r:0};this.QSM={cN:"string",b:'"',e:'"',i:"\\n",c:[this.BE],r:0};this.CLCM={cN:"comment",b:"//",e:"$"};this.CBLCLM={cN:"comment",b:"/\\*",e:"\\*/"};this.HCM={cN:"comment",b:"#",e:"$"};this.NM={cN:"number",b:this.NR,r:0};this.CNM={cN:"number",b:this.CNR,r:0};this.BNM={cN:"number",b:this.BNR,r:0};this.inherit=function(r,s){var p={};for(var q in r){p[q]=r[q]}if(s){for(var q in s){p[q]=s[q]}}return p}}();hljs.LANGUAGES.cpp=function(){var a={keyword:{"false":1,"int":1,"float":1,"while":1,"private":1,"char":1,"catch":1,"export":1,virtual:1,operator:2,sizeof:2,dynamic_cast:2,typedef:2,const_cast:2,"const":1,struct:1,"for":1,static_cast:2,union:1,namespace:1,unsigned:1,"long":1,"throw":1,"volatile":2,"static":1,"protected":1,bool:1,template:1,mutable:1,"if":1,"public":1,friend:2,"do":1,"return":1,"goto":1,auto:1,"void":2,"enum":1,"else":1,"break":1,"new":1,extern:1,using:1,"true":1,"class":1,asm:1,"case":1,typeid:1,"short":1,reinterpret_cast:2,"default":1,"double":1,register:1,explicit:1,signed:1,typename:1,"try":1,"this":1,"switch":1,"continue":1,wchar_t:1,inline:1,"delete":1,alignof:1,char16_t:1,char32_t:1,constexpr:1,decltype:1,noexcept:1,nullptr:1,static_assert:1,thread_local:1,restrict:1,_Bool:1,complex:1},built_in:{std:1,string:1,cin:1,cout:1,cerr:1,clog:1,stringstream:1,istringstream:1,ostringstream:1,auto_ptr:1,deque:1,list:1,queue:1,stack:1,vector:1,map:1,set:1,bitset:1,multiset:1,multimap:1,unordered_set:1,unordered_map:1,unordered_multiset:1,unordered_multimap:1,array:1,shared_ptr:1}};return{dM:{k:a,i:"</",c:[hljs.CLCM,hljs.CBLCLM,hljs.QSM,{cN:"string",b:"'\\\\?.",e:"'",i:"."},{cN:"number",b:"\\b(\\d+(\\.\\d*)?|\\.\\d+)(u|U|l|L|ul|UL|f|F)"},hljs.CNM,{cN:"preprocessor",b:"#",e:"$"},{cN:"stl_container",b:"\\b(deque|list|queue|stack|vector|map|set|bitset|multiset|multimap|unordered_map|unordered_set|unordered_multiset|unordered_multimap|array)\\s*<",e:">",k:a,r:10,c:["self"]}]}}}();hljs.LANGUAGES.r={dM:{c:[hljs.HCM,{cN:"number",b:"\\b0[xX][0-9a-fA-F]+[Li]?\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"number",b:"\\b\\d+(?:[eE][+\\-]?\\d*)?L\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"number",b:"\\b\\d+\\.(?!\\d)(?:i\\b)?",e:hljs.IMMEDIATE_RE,r:1},{cN:"number",b:"\\b\\d+(?:\\.\\d*)?(?:[eE][+\\-]?\\d*)?i?\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"number",b:"\\.\\d+(?:[eE][+\\-]?\\d*)?i?\\b",e:hljs.IMMEDIATE_RE,r:1},{cN:"keyword",b:"(?:tryCatch|library|setGeneric|setGroupGeneric)\\b",e:hljs.IMMEDIATE_RE,r:10},{cN:"keyword",b:"\\.\\.\\.",e:hljs.IMMEDIATE_RE,r:10},{cN:"keyword",b:"\\.\\.\\d+(?![\\w.])",e:hljs.IMMEDIATE_RE,r:10},{cN:"keyword",b:"\\b(?:function)",e:hljs.IMMEDIATE_RE,r:2},{cN:"keyword",b:"(?:if|in|break|next|repeat|else|for|return|switch|while|try|stop|warning|require|attach|detach|source|setMethod|setClass)\\b",e:hljs.IMMEDIATE_RE,r:1},{cN:"literal",b:"(?:NA|NA_integer_|NA_real_|NA_character_|NA_complex_)\\b",e:hljs.IMMEDIATE_RE,r:10},{cN:"literal",b:"(?:NULL|TRUE|FALSE|T|F|Inf|NaN)\\b",e:hljs.IMMEDIATE_RE,r:1},{cN:"identifier",b:"[a-zA-Z.][a-zA-Z0-9._]*\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"operator",b:"<\\-(?!\\s*\\d)",e:hljs.IMMEDIATE_RE,r:2},{cN:"operator",b:"\\->|<\\-",e:hljs.IMMEDIATE_RE,r:1},{cN:"operator",b:"%%|~",e:hljs.IMMEDIATE_RE},{cN:"operator",b:">=|<=|==|!=|\\|\\||&&|=|\\+|\\-|\\*|/|\\^|>|<|!|&|\\||\\$|:",e:hljs.IMMEDIATE_RE,r:0},{cN:"operator",b:"%",e:"%",i:"\\n",r:1},{cN:"identifier",b:"`",e:"`",r:0},{cN:"string",b:'"',e:'"',c:[hljs.BE],r:0},{cN:"string",b:"'",e:"'",c:[hljs.BE],r:0},{cN:"paren",b:"[[({\\])}]",e:hljs.IMMEDIATE_RE,r:0}]}};
hljs.initHighlightingOnLoad();
</script>

<!-- MathJax scripts -->
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


<style type="text/css">
body, td {
   font-family: sans-serif;
   background-color: white;
   font-size: 13px;
}

body {
  max-width: 800px;
  margin: auto;
  padding: 1em;
  line-height: 20px;
}

tt, code, pre {
   font-family: 'DejaVu Sans Mono', 'Droid Sans Mono', 'Lucida Console', Consolas, Monaco, monospace;
}

h1 {
   font-size:2.2em;
}

h2 {
   font-size:1.8em;
}

h3 {
   font-size:1.4em;
}

h4 {
   font-size:1.0em;
}

h5 {
   font-size:0.9em;
}

h6 {
   font-size:0.8em;
}

a:visited {
   color: rgb(50%, 0%, 50%);
}

pre, img {
  max-width: 100%;
}
pre {
  overflow-x: auto;
}
pre code {
   display: block; padding: 0.5em;
}

code {
  font-size: 92%;
  border: 1px solid #ccc;
}

code[class] {
  background-color: #F8F8F8;
}

table, td, th {
  border: none;
}

blockquote {
   color:#666666;
   margin:0;
   padding-left: 1em;
   border-left: 0.5em #EEE solid;
}

hr {
   height: 0px;
   border-bottom: none;
   border-top-width: thin;
   border-top-style: dotted;
   border-top-color: #999999;
}

@media print {
   * {
      background: transparent !important;
      color: black !important;
      filter:none !important;
      -ms-filter: none !important;
   }

   body {
      font-size:12pt;
      max-width:100%;
   }

   a, a:visited {
      text-decoration: underline;
   }

   hr {
      visibility: hidden;
      page-break-before: always;
   }

   pre, blockquote {
      padding-right: 1em;
      page-break-inside: avoid;
   }

   tr, img {
      page-break-inside: avoid;
   }

   img {
      max-width: 100% !important;
   }

   @page :left {
      margin: 15mm 20mm 15mm 10mm;
   }

   @page :right {
      margin: 15mm 10mm 15mm 20mm;
   }

   p, h2, h3 {
      orphans: 3; widows: 3;
   }

   h2, h3 {
      page-break-after: avoid;
   }
}
</style>



</head>

<body>
<hr/>

<h2>output: pdf_document</h2>

<!-- 
%\VignetteEngine{knitr::knitr} 
%\VignetteIndexEntry{An Introduction to EBglmnet}
--> 

<p><a id="top"></a></p>

<h1>EBglmnet Vignette</h1>

<h3>Anhui Huang and Dianting Liu</h3>

<h4>Jan. 15, 2016</h4>

<blockquote>
<p><a href="#intro">Introduction</a></p>

<p><a href="#install">Installation</a></p>

<p><a href="#qs">Quick Start</a></p>

<p><a href="#glm">GLM Family</a></p>

<p><a href="#prior">Prior, Hyperparameters and Epistasis</a></p>

<p><a href="#example">Example of p&gt;n Data</a></p>

<p><a href="#parallel">Parallel Cross Validation</a></p>
</blockquote>

<p><a id="intro"></a></p>

<h2>Introduction</h2>

<h4>Acronyms to be used</h4>

<p>EBglmnet is a package that implements the empirical Bayesian Lasso (EBlasso) and Elastic Net (EBEN) method for generalized linear models (GLMs). Additionally, in <code>EBlasso</code>, two different prior distributions are also developed: one with two-level hierarchical Normal + Exponential prior (denoted as <code>NE</code>), and the other one with three-level Normal + Exponential + Gamma prior (denoted as <code>NEG</code>). The following names should not be confused with the <code>lasso</code> and <code>elastic net</code>  method from the comparison package <code>glmnet</code>:</p>

<p><code>EBglmnet</code>: package that implements <code>EBlasso</code> and <code>EBEN</code> methods.</p>

<p><code>EBlasso</code>: empirical Bayesian method with <code>lasso</code> prior distribution, which includes two sets of prior distributions: <code>NE</code> and <code>NEG</code>.</p>

<p><code>EBEN</code>: empirical Bayesian method with <code>elastic net</code> prior distribution.</p>

<p><code>lasso</code> prior: the hierarchical prior distribution that is equivalent with <code>lasso</code> penalty term when the marginal probability distribution for the regression coefficients is considered.</p>

<p><code>elastic net</code> prior: the hierarchical prior distribution that is equivalent with <code>elastic net</code> penalty term when the marginal probability distribution for the regression coefficients is considered.</p>

<p><code>EBlasso-NE</code>: <code>EBlasso</code> method with <code>NE</code> prior.</p>

<p><code>EBlasso-NEG</code>: <code>EBlasso</code> method with <code>NEG</code> prior.</p>

<h4>Generalized Linear Models (GLMs)</h4>

<p>In a GLM </p>

<p>\[
 \boldsymbol{\eta}=\mu\mathit{\boldsymbol{I}}+\mathbf{X}\boldsymbol{\beta},
\]
where \(\bf X\) is an \(\it{n}\times\it{p}\) matrix containing \(\it{p}\) variables for \(\it{n}\) samples (\(\it{p}\) can be \(\gg \it{n}\)). \(\boldsymbol{\eta}\) is an \(\it{n}\times 1\) linear predictor and is related to the response variable  \(\mathit{\boldsymbol{y}}\) through a link function \(\it{g}\): E(\(\mathit{\mathbf{y}} |\bf X\))=\(\it{g}^{-1}\)(\(\mu\mathit{\boldsymbol{I}} +\mathbf{X}\boldsymbol{\beta}\)), and \(\boldsymbol{\beta}\) is a \(\it{p}\times 1\) vector of regression coefficients.
Depending on certain assumption of the data distribution on \(\mathit{\boldsymbol{y}}\), the GLM is generally inferred through finding the set of model parameters that maximize the model likelihood function  \(\mathit{p}\)(\(\mathit{\boldsymbol{y}}|\mu, \boldsymbol{\beta}, \varphi\)), where \(\varphi\) denotes the other model parameters of the data distribution. However, such Maximum Likelihood (ML) approach is no longer applicable when \(\it{p}\gg \it{n}\). With Bayesian lasso and Bayesian elastic net (EN) prior distribution on \(\boldsymbol{\beta}\), <code>EBglmnet</code> solves the problem by inferring a sparse posterior distribution for  \(\hat{\boldsymbol{\beta}}\), which includes exactly zero regression coefficients for irrelevant variables and both posterior mean and variance for non-zero ones. Comparing with the <code>glmnet</code> package, not only does <code>EBglmnet</code> provide features including both sparse outcome and hypothesis testing, simulation study and real data analysis in the reference papers also demonstrates the better performance in terms of Power of Detection (PD), False Discovery Rate (FDR), as well as Power Detecting Group Effects when applicable. While mathematical details of the <code>EBlasso</code> and <code>EBEN</code> methods can be found in the reference papers, the principle of the methods and differences on the prior distributions will be briefly introduced here.</p>

<h3>Lasso and its Bayesian Interpretation</h3>

<p>Lasso applies a penalty term on the log likelihood function and solves for \(\hat{\boldsymbol{\beta}}\) by maximizing the following penalized likelihood:</p>

<p>\[
\hat{\boldsymbol{\beta}} = \arg_{\boldsymbol{\beta}}\max\left[\log\mathit{p}(\mathit{\boldsymbol{y}}|\mu, \boldsymbol{\beta}, \varphi) -\lambda||\boldsymbol{\beta}||_1\right].
\]</p>

<p>The \(\it{l_1}\) penalty term can be regarded as a mixture of hierarchical prior distribution:</p>

<p>\[
\beta_j  \sim \mathit{N}(0,\sigma_j^2),\
\sigma_j^2  \sim \exp(\lambda), j = 1, \dots, p,
\]</p>

<p>and maximizing the penalized likelihood function is equivalent to maximizing the marginal posterior distribution of \(\boldsymbol{\beta}\) :</p>

<p>\[
\hat{\boldsymbol{\beta}} = \arg_{\boldsymbol{\beta}}\max \log \mathit{p}(\boldsymbol{\beta}|\mathit{\boldsymbol{y}},\mathbf{X},\mu,\lambda, \varphi)\
\approx\arg_{\boldsymbol{\beta}}\max \log\int\left[\mathit{p}(\mathit{\boldsymbol{y}}|\mu, \boldsymbol{\beta}, \varphi)\cdot (2\pi)^{-p/2}\lvert\mathbf{A}\rvert^{&frac12;}\exp\{-\frac{1}{2}\boldsymbol{\beta}^T\mathbf{A}\boldsymbol{\beta}\}\cdot \prod^p_{j=1}\lambda\exp\{-\lambda\sigma_j^2\}\right]d\boldsymbol{\sigma}^2,
\]</p>

<p>where \(\mathbf{A}\) is a diagonal matrix with \(\boldsymbol{\sigma}^{-2}\) on its diagonal. Of note, <code>lasso</code> integrates out the variance information \(\boldsymbol{\sigma}^2\) and estimates a posterior mode \(\hat{\boldsymbol{\beta}}\). The \(\it{l_1}\) penalty ensures that a sparse solution can be achieved. In Bayesian lasso (Park and Casella, 2008), the prior probability distribution is also conditional on the residual variance so that it has a unique mode.</p>

<h3>Empirical Bayesian Lasso (<code>EBlasso</code>)</h3>

<p><code>EBglmnet</code> keeps the variance information, while still enjoying the sparse property by taking a different and slightly complicated approach as showed below using <code>EBlasso-NE</code> as an example:</p>

<p>In contrary to the marginalization on  \(\boldsymbol{\beta}\), the first step in <code>EBlasso-NE</code> is to obtain a marginal posterior distribution for  \(\boldsymbol{\sigma}^2\) :</p>

<p>\[
\mathit{p}(\boldsymbol{\sigma}^2|\mathit{\boldsymbol{y}},\mathbf{X},\mu,\lambda, \varphi) = \int c \left[\mathit{p}(\mathit{\boldsymbol{y}}|\mu, \boldsymbol{\beta}, \varphi)\cdot (2\pi)^{-p/2}\lvert\mathbf{A}\rvert^{&frac12;}\exp\{-\frac{1}{2}\boldsymbol{\beta}^T\mathbf{A}\boldsymbol{\beta}\}\cdot \prod^p_{j=1}\lambda\exp\{-\lambda\sigma_j^2\}\right]d\boldsymbol{\beta},
\]</p>

<p>where \(c\) is a constant.
While the integral in <code>lasso</code> is achieved through the conjugated Normal + Exponential (NE) prior, the integral in <code>EBlasso-NE</code> is completed through mixture of two normal distributions:  \(\it{p}\)(\(\boldsymbol{\beta}|\boldsymbol{\sigma}^2\)) and  \(\mathit{p}\)(\(\mathit{\boldsymbol{y}}|\mu, \boldsymbol{\beta}, \varphi\)), and the latter one is typically approximated to a normal distribution through Laplace approximation if itself is not a normal PDF. Then the estimate \(\hat{\boldsymbol{\sigma}}^2\) can be obtained by maximizing this marginal posterior distribution, which has the following form:</p>

<p>\[
\hat{\boldsymbol{\sigma}}^2 = \arg_{\boldsymbol{\sigma}^2}\max \log \mathit{p}(\boldsymbol{\sigma}^2|\mathit{\boldsymbol{y}},\mathbf{X},\mu,\lambda, \varphi)\
= \arg_{\boldsymbol{\sigma}^2}\max \left[
\log\mathit{p}(\mathit{\boldsymbol{y}}|\mu, \boldsymbol{\sigma}^2, \varphi,\lambda)-\sum^p_{j=1}\lambda\sigma_j^2 + c\right].
\]</p>

<p>Given the constraint that \(\boldsymbol{\sigma}^2 > 0\), the above equation essentially maximizes the \(\it{l_1}\) penalized marginal likelihood function of \(\boldsymbol{\sigma}^2\), which images the \(\it{l_1}\) penalty in <code>lasso</code> with the beauty of producing a sparse solution for \(\hat{\boldsymbol{\sigma}}^2\). Note that if \(\hat{\sigma}_j^2 = 0\), \(\hat{\beta}_j\) will also be zero and variable \(\mathit{x}_j\) will be excluded from the model.  Finally, with the sparse estimate of \(\hat{\boldsymbol{\sigma}}^2\), the posterior estimate of \(\hat{\boldsymbol{\beta}}\) and other nuance parameters can then be obtained accordingly.</p>

<h3>Hierarchical Prior Distributions in <code>EBglmnet</code></h3>

<h4>Prior 1: EBlasso-NE</h4>

<p>\[
\beta_j  \sim \mathit{N}(0,\sigma_j^2),\
\sigma_j^2  \sim \exp(\lambda), j = 1, \dots, p
\]</p>

<p>As illustrated above, assuming a Normal + Exponential hierarchical prior distribution on \(\boldsymbol{\beta}\) (<code>EBlasso-NE</code>) will yield exactly the lasso prior. <code>EBlasso-NE</code> accommodates the properties of sparse solution and hypothesis testing given both the estimated mean and variance information in  \(\hat{\boldsymbol{\beta}}\) and \(\hat{\boldsymbol{\sigma}}^2\).  The NE prior is &ldquo;peak zero and flat tails&rdquo;, which can select variables with relatively small effect size while shrinking most of irrelevant effects to exactly zero. <code>EBlasso-NE</code> can be applied to natural population analysis when effect sizes are relatively small.</p>

<h4>Prior 2: EBlasso-NEG</h4>

<p>The prior in <code>EBlasso-NE</code> has a relatively large probability mass on the nonzero tails, resulting in a large number of non-zero small effects with large \(p\)-values in simulation and real data analysis. We further developed another well studied conjugated hierarchical prior distribution under the empirical Bayesian framework, the Normal + Exponential + Gamma (NEG) prior:</p>

<p>\[
\beta_j  \sim \mathit{N}(0,\sigma_j^2),\
\sigma_j^2  \sim \exp(\lambda),  \
\lambda \sim gamma(a,b), j = 1, \dots,p
\]</p>

<p>Comparing with <code>EBlasso-NE</code>, the NEG prior has a larger probability centered at 0, and will only yield nonzero regression coefficients for effects having relatively large signal to noise ratio. </p>

<h4>Prior 3: Elastic Net Prior for Grouping Effect</h4>

<p>Similar to <code>lasso</code>, <code>EBlasso</code> typically selects one variable out of a group of correlated variables. While <code>elastic net</code> was developed to encourage a grouping effect by incorporating an \(\it{l_2}\) penalty term, <code>EBglmnet</code> implemented an innovative <code>elastic net</code> hierarchical prior:</p>

<p>\[
\beta_j  \sim \mathit{N}\left[0,(\lambda_1 + \tilde{\sigma}_j^{-2})^{-1}\right], \
\tilde{\sigma_j}^2  \sim generalized\ gamma(\lambda_1,\lambda_2), j = 1, \dots, p.
\]</p>

<p>The generalized gamma distribution has PDF: \(f(\tilde{\sigma_j}^2|\lambda_1, \lambda_2) = c(\lambda_1\tilde{\sigma_j}^2 + 1)^{-&frac12;}\exp\{-\lambda_2\tilde{\sigma_j}^2\}, j=1,\dots,p\), with c being a normalization constant. The property of this prior can be appreciated from the following aspects:</p>

<h5>(1): \(\lambda_1 = 0\)</h5>

<p>When \(\lambda_1 = 0\), the generalized gamma distribution becomes an exponential distribution: \(f(\tilde{\sigma_j}^2|\lambda_2) = c\exp\{-\lambda_2\tilde{\sigma_j}^2\}, j=1,\dots,p\), with \(c= \lambda_2\), and the elastic net prior is reduced to the two level <code>EBlasso-NE</code> prior.</p>

<h5>(2): \(\lambda_1 > 0\)</h5>

<p>When \(\lambda_1 > 0\), the generalized gamma distribution can be written as a shift gamma distribution having the following PDF: \(f(\tilde{\sigma_j}^2|a,b,\gamma) = \frac{b^a}{\Gamma(a)}(\tilde{\sigma_j}^2 - \gamma)^{a-1}\exp\{-b(\tilde{\sigma_j}^2 - \gamma)\}\), where \(a=&frac12;, b = \lambda_2\), and \(\gamma = -1/\lambda_1\). In (Huang A. 2015), it is proved that the marginal prior distribution of \(\beta_j\) can be obtained as \(p(\beta_j)\propto\exp\{-\frac{\lambda_1}{2}\beta_j^2 - \sqrt{2\lambda_2}|\beta_j|\}\), which is equivalent to the <code>elastic net</code> method in <code>glmnet</code>.  </p>

<h5>(3): structure of \(\boldsymbol{\sigma}^2\) and interpretation of the <code>elastic net</code> prior</h5>

<p>Note that the prior variance for the regression coefficients has this form: \(\boldsymbol{\sigma}^2=\tilde{\boldsymbol{\sigma}}^{2}/(\lambda_1\tilde{\boldsymbol{\sigma}}^2+\mathit{\boldsymbol{I}})\). This structure seems counter-intuitive at first glance. However, if we look at it from precision point of view, i.e.,  \(\boldsymbol{\alpha} = \boldsymbol{\sigma}^{-2}\), and \(\tilde{\boldsymbol{\alpha}} = \tilde{\boldsymbol{\sigma}}^{-2}\), then we have:</p>

<p>\[
\boldsymbol{\alpha} =\lambda_1\mathit{\boldsymbol{I}} +  \tilde{\boldsymbol{\alpha}}.
\]</p>

<p>The above equation demonstrates that we actually decompose the precision of the normal prior into a fixed component \(\lambda_1\) shared by all explanatory variables and a random component \(\tilde{\boldsymbol{\alpha}}\) that is unique for each explanatory variable. This design represents the mathematical balance between the inter-group independence and intra-group correlation among explanatory variables, and is aligned with the objective of sparseness while encouraging grouping effects.</p>

<p>The empirical Bayesian elastic net (EBEN) in <code>EBglmnet</code> is solved similarly as <code>EBlasso</code> using the aforementioned empirical Bayesian approach. Research studies presented in the reference papers demonstrated that <code>EBEN</code> has better performance comparing with <code>elastic net</code>, in terms of PD, FDR, and most importantly, Power of Detecting Groups. </p>

<h3><code>EBglmnet</code> Implementation and Usage</h3>

<p>The <code>EBglmnet</code> algorithms use greedy coordinate descent, which successively optimizes the objective function over each parameter with others fixed, and cycles repeatedly until convergence. Key algorithms are implemented in C/C++ with matrix computation using the BLAS/LAPACK packages.  Due to closed form solutions for \(\hat{\boldsymbol{\sigma}}^2\) in all prior setups and other algorithmic and programming techniques, the algorithms can compute the solutions very fast.</p>

<p>We recommend using <code>EBlasso-NEG</code> when there are a large number of candidate effects (eg., \(\ge 10^6\) number of effects such as whole-genome epistasis analysis and GWAS), and using <code>EBEN</code> when groups of highly correlated variables are of interest.</p>

<p>The authors of <code>EBglmnet</code> are Anhui Huang  and Dianting Liu. This vignette describes the principle and usage of <code>EBglmnet</code> in R. Users are referred to the papers in the Reference section for details of the algorithms.</p>

<p><a id="install"></a></p>

<h2>Installation</h2>

<p><code>EBglmnet</code> can be installed directly from CRAN using the following command in R console:</p>

<pre><code class="r">install.packages(&quot;EBglmnet&quot;, repos = &quot;http://cran.us.r-project.org&quot;)
</code></pre>

<p>which will download and install the package to the default directory. Alternatively, users can download the pre-compiled binary file at <a href="http://cran.r-project.org/web/packages/EBglmnet/index.html">http://cran.r-project.org/web/packages/EBglmnet/index.html</a>, and install it from local package.</p>

<p><a href="#top">Back to Top</a></p>

<p><a id="qs"></a></p>

<h2>Quick Start</h2>

<p>We will give users a general idea of the package by using a simple example that demonstrates the basis package usage. Through running the main functions and examining the outputs, users may have a better idea on how the package works, what functions are available, which parameters to choose, as well as where to seek help. More details are given in later sections.</p>

<p>Let us first clear up the workspace and load the <code>EBglmnet</code> package:</p>

<pre><code class="r">rm(list = ls())
library(EBglmnet)
</code></pre>

<p>We will use an R built-in dataset <code>state.x77</code> as an example, which includes a matrix with 50 rows and 8 columns giving the following measurements in the respective columns: Population, Income, Illiteracy, Life Expectancy, Murder Rate, High School Graduate Rate, Days Below Freezing Temperature, and Land Area. The default model used in the package is the Gaussian linear model, and we will demonstrate it using Life Expectancy as the response variable and the remaining as explanatory variables. We create the input data as showed below, and users can load their own data and prepare variable <code>y</code> and <code>x</code> following this example.</p>

<pre><code class="r">varNames = colnames(state.x77);
varNames
</code></pre>

<pre><code>## [1] &quot;Population&quot; &quot;Income&quot;     &quot;Illiteracy&quot; &quot;Life Exp&quot;   &quot;Murder&quot;    
## [6] &quot;HS Grad&quot;    &quot;Frost&quot;      &quot;Area&quot;
</code></pre>

<pre><code class="r">y= state.x77[,&quot;Life Exp&quot;]
xNames = c(&quot;Population&quot;,&quot;Income&quot;,&quot;Illiteracy&quot;, &quot;Murder&quot;,&quot;HS Grad&quot;,&quot;Frost&quot;,&quot;Area&quot;)
x = state.x77[,xNames]
</code></pre>

<p>We fit the model using the most basic call to  <code>EBglmnet</code> with default prior</p>

<pre><code class="r">set.seed(1)
output = EBglmnet(x,y,hyperparameters = c(0.1, 0.1))
</code></pre>

<p>&ldquo;output&rdquo; is a list containing all the relevant information of the fitted model. Users can examine the output by directly looking at each element in the list. Particularly, the sparse regression coefficients can be extracted as showed below:</p>

<pre><code class="r">glmfit = output$fit
variables = xNames[glmfit[,1,drop=FALSE]]
cbind(variables,as.data.frame(round(glmfit[,3:6,drop=FALSE],4)))
</code></pre>

<pre><code>##   variables    beta posterior variance t-value p-value
## 1    Murder -0.2716              2e-04 19.1011       0
</code></pre>

<p>The hyperparameters in each of the prior distributions control the number of non-zero effects to be selected, and  cross-validation (CV) is perhaps the simplest and most widely used method in deciding their values. <code>cv.EBglmnet</code> is the main function to do cross-validation, which can be called using the following code.</p>

<pre><code class="r">cvfit = cv.EBglmnet(x, y)
</code></pre>

<pre><code>## EBLASSO Linear Model, NEG prior,Epis:  FALSE ; 5 fold cross-validation
</code></pre>

<p><code>cv.EBglmnet</code> returns a <code>cv.EBglmnet</code> object, which is a list with all the ingredients of CV and the final fit results using CV selected hyperparameters. We can view the CV results, selected hyperparameters and the corresponding coefficients. For example, result using different hyperparameters and the corresponding prediction errors are shown below:</p>

<pre><code class="r">cvfit$CrossValidation
</code></pre>

<pre><code>##           a    b Mean Square Error standard error
##  [1,]  0.01 0.01         0.7930915      0.7329892
##  [2,]  0.05 0.05         0.8673170      0.7273493
##  [3,]  0.10 0.10         0.8858297      0.6849729
##  [4,]  0.50 0.50         0.8787371      0.5705269
##  [5,]  1.00 1.00         1.3098721      0.7438981
##  [6,] -0.01 0.01         0.7930823      0.7337260
##  [7,] -0.10 0.01         0.7205029      0.4003129
##  [8,] -0.20 0.01         0.8058868      0.4886126
##  [9,] -0.30 0.01         0.9844774      0.8831515
## [10,] -0.40 0.01         0.8247224      0.5932214
## [11,] -0.50 0.01         0.8603200      0.5962155
## [12,] -0.60 0.01         0.8672803      0.6027390
## [13,] -0.70 0.01         0.9110442      0.6710180
## [14,] -0.80 0.01         0.9106710      0.6728358
## [15,] -0.90 0.01         0.9108844      0.6773827
## [16,] -0.10 0.05         0.8714122      0.5887262
## [17,] -0.10 0.10         0.9880775      0.7285293
## [18,] -0.10 0.50         0.9132672      0.5987704
## [19,] -0.10 1.00         0.9822634      0.6155989
</code></pre>

<p>The selected parameters and the corresponding fitting results are:</p>

<pre><code class="r">cvfit$hyperparameters
</code></pre>

<pre><code>##     a     b 
## -0.10  0.01
</code></pre>

<pre><code class="r">cvfit$fit
</code></pre>

<pre><code>##      locus1 locus2       beta posterior variance   t-value      p-value
## [1,]      4      4 -0.2598186       6.244212e-04 10.397560 5.440093e-14
## [2,]      5      5  0.0129420       1.417103e-05  3.437959 1.204829e-03
</code></pre>

<p><a href="#top">Back to Top</a></p>

<p><a id="glm"></a></p>

<h2>GLM Family</h2>

<p>Two families of models have been developed in <code>EBglmnet</code>, the  <code>gaussian</code> family and the <code>binomial</code> family, which are essentially different probability distribution assumptions on the response variable <code>y</code>.</p>

<h3>Gaussian Model</h3>

<p><code>EBglmnet</code> assumes a Gaussian distribution on <code>y</code> by default, i.e., \(\mathit{p}(\mathit{\boldsymbol{y}}|\mu, \boldsymbol{\beta}, \varphi) = N( \mu\mathit{\boldsymbol{I}} + \mathbf{X}\boldsymbol{\beta}, \sigma_0^2\boldsymbol{I})\), where \(\varphi= \sigma_0^2\) is the residual variance. In the above example, both \(\hat{\mu}\) and \(\hat{\sigma_0}^2\) are listed in the output: </p>

<pre><code class="r">output$Intercept
</code></pre>

<pre><code>## [1] 72.88376
</code></pre>

<pre><code class="r">output$residual
</code></pre>

<pre><code>## [1] 0.6912821
</code></pre>

<h3>Binomial Model</h3>

<p>If there are two possible outcomes in the response variable, a binomial distribution assumption on <code>y</code> is available in <code>EBglmnet</code>, which has \(\mathit{p}(\mathit{\boldsymbol{y}}|\mu, \boldsymbol{\beta}, \varphi)\) following a binomial distribution and \(\varphi\in\emptyset\). Same as the widely-used logistic regression model, the link function is \(\eta_i = logit(p_i)=\log(\frac{Pr(y_i)=1}{1-Pr(y_i=1)}), i = 1, \dots, n\). To run <code>EBglmnet</code> with binomial models, users need to specify the parameter <code>family</code> as <code>binomial</code>:</p>

<pre><code class="r">yy = y&gt;mean(y);
output = EBglmnet(x,yy,family=&quot;binomial&quot;, hyperparameters = c(0.1, 0.1))
</code></pre>

<p>For illustration purpose, the above codes created a binary variable <code>yy</code> by set the cutoff at the mean Life Expectancy value. </p>

<p><a href="#top">Back to Top</a></p>

<p><a id="prior"></a></p>

<p>##Prior, Hyperparameters and Epistasis</p>

<p>The three sets of hierarchical prior distribution can be specified by <code>prior</code> option in <code>EBglmnet</code>. By default, <code>EBglmnet</code> assumes the <code>lassoNEG</code> prior. The following example changes the prior via <code>prior</code> parameter:</p>

<pre><code class="r">output = EBglmnet(x,yy,family=&quot;binomial&quot;, prior = &quot;elastic net&quot;, hyperparameters = c(0.1, 0.1))
</code></pre>

<p>Note that the hyperparameters setup is associated with a specific prior. In <code>lasso</code> prior, only one hyperparameter \(\lambda\) is required, while in <code>elastic net</code> and <code>lassoNEG</code>, two hyperparameters need to be specified. For <code>EBEN</code> having the <code>elastic net</code> prior distribution, the two hyperparameters \(\lambda_1\) and \(\lambda_2\) are defined in terms of other two parameters \(\alpha \in [0,1]\) and \(\lambda>0\) same as in <code>glmnet</code> package, such that \(\lambda_1 = (1-\alpha)\lambda\) and  \(\lambda_2 = \alpha\lambda\). Therefore, users are asked to specify \(hyperparameters=c(\alpha, \lambda)\). </p>

<p>In genetic and population analysis, sometimes it is interested in analyzing the interaction terms among the variables (epistasis). <code>EBglmnet</code> provides a feature that can incorporate all pair-wise interactions into analysis, which is achieved by setting <code>Epis</code> as <code>TRUE</code>:</p>

<pre><code class="r">output = EBglmnet(x,yy,family=&quot;binomial&quot;, prior = &quot;elastic net&quot;, 
                  hyperparameters = c(0.1, 0.1),Epis = TRUE)
output$fit
</code></pre>

<pre><code>##      locus1 locus2          beta posterior variance   t-value    p-value
## [1,]      4      4 -5.318341e-02       1.491454e-03 1.3771184 0.17473457
## [2,]      1      7  1.719555e-10       5.184252e-20 0.7552192 0.45373244
## [3,]      2      4 -8.894085e-06       6.408661e-11 1.1110091 0.27198645
## [4,]      3      4 -3.785224e-02       4.632023e-04 1.7587586 0.08486232
## [5,]      4      5 -3.100472e-04       2.447304e-07 0.6267348 0.53374233
## [6,]      4      6 -4.688616e-04       1.273471e-07 1.3138633 0.19501041
</code></pre>

<p>When <code>Epis = TRUE</code>, both \(p\) number of main effects and \(p(p-1)/2\) number of interaction effects are considered in the model. In the output, <code>locus1</code> and <code>locus2</code> denote the pair of interaction variables, and if the numbers are the same, the corresponding effect is from a main effect.  Users should be aware of the significant larger number variables (i.e., \(p(p-1)/2\) more variables), thus a longer time to finish the computation.</p>

<p><a href="#top">Back to Top</a></p>

<p><a id="example"></a></p>

<p>##Example of p&gt;n Data</p>

<h3>Gaussian Model with Main Effects</h3>

<p>We will demontrate the application of EBglmnet in multiple QTL mapping using a simulated F2 population, which is available along with EBglmnet package. The genotype of the F2 population is simulated  from cross of two inbred lines. A total of \(p=481\) gentic markers were simulated to be evenly spaced on a large chromosome of 2400 centi-Morgan (cM) with an interval of d = 5 cM. Theoretically, two adjacent markers have a correlation coefficient \(R = e^{-2d} = 0.9048\) since the Haldane map function is assumed. The dummy variable for the three genotypes, AA, Aa and aa of individual i at marker j was defined as \(x_{ij} = 1, 0, -1\), respectively.  </p>

<pre><code class="r">data(BASIS)#this is the genotype of the the F2 population
N = nrow(BASIS)
p = ncol(BASIS)
j = sample((p-2),1)
cor(BASIS[,c(j,j+1,j+2)]) #Correlation structure among neighboring markers
</code></pre>

<pre><code>##           m229      m230      m231
## m229 1.0000000 0.9038431 0.8143151
## m230 0.9038431 1.0000000 0.9023130
## m231 0.8143151 0.9023130 1.0000000
</code></pre>

<p>Let us first simulate a quantitative phenotype with population mean 100 and residual variance that contribute to 10% of population variance. Ten QTLs were simulated from the 481 markers, and effect sizes are randomly generated from [2,3]. We assume that QTLs were coincided with markers. If QTLs were not on markers, they may still be detected given the above correlation structure, although a slightly larger sample size may be needed to give the same PD:</p>

<pre><code class="r">set.seed(1);
Mu = 100; #population mean;
nTrue = 10; # we assume 10 out of the 481 effects are true QTLs
trueLoc = sort(sample(p,nTrue));
trueEff = runif(nTrue,2,3); #effect size from 2-3
xbeta = BASIS[,trueLoc]%*%trueEff;
s2 =  var(xbeta)*0.1/0.9 #residual variance with 10% noise
residual = rnorm(N,mean=0,sd=sqrt(s2))
y = Mu + xbeta + residual;  
</code></pre>

<p>To demonstrate the performance of EBglmnet in analyzing dataset with \(p>n\), we will analyze the simulated dataset using a smaller sample size \(n=300\):</p>

<pre><code class="r">n = 300;
index = sample(N,n); 
CV = cv.EBglmnet(x=BASIS[index,],y=y[index],family=&quot;gaussian&quot;,prior= &quot;lassoNEG&quot;,nfold= 5)
</code></pre>

<pre><code>## EBLASSO Linear Model, NEG prior,Epis:  FALSE ; 5 fold cross-validation
</code></pre>

<p>With 5 fold CV, EBlasso-NEG identified the following effects:</p>

<pre><code class="r">CV$fit
</code></pre>

<pre><code>##       locus1 locus2     beta posterior variance  t-value p-value
##  [1,]     30     30 2.527794         0.03160368 14.21912       0
##  [2,]     97     97 2.347208         0.03473698 12.59376       0
##  [3,]    128    128 2.466978         0.03685331 12.85071       0
##  [4,]    178    178 2.461466         0.03379284 13.39003       0
##  [5,]    275    275 2.389084         0.03847676 12.17958       0
##  [6,]    298    298 2.434124         0.03277078 13.44619       0
##  [7,]    314    314 2.340964         0.03717382 12.14161       0
##  [8,]    428    428 2.737366         0.04559953 12.81896       0
##  [9,]    435    435 2.316184         0.05005450 10.35265       0
## [10,]    449    449 3.070134         0.03764603 15.82332       0
</code></pre>

<pre><code class="r">trueLoc
</code></pre>

<pre><code>##  [1]  30  97 128 179 275 298 314 428 435 449
</code></pre>

<p>Comparing with the true QTL locations, EBlasso-NEG successfully identified all QTLs. Of note, an identified marker that is &lt;20cM from a true QTL is generally not considered as a false effect, given the small distance and high genotype correlation. Since there is limited prior information in this simulation, other methods in EBglmnet will yield similar results. </p>

<h3>Binomial Model and Separation Problem</h3>

<p>In many genetics and population analysis such as binary QTL mapping or GWAS, both genetic effects (eg., \(\mathbf{X}\) takes values of 1, 0, -1 denoting three genotype values AA, Aa, aa) and response variable (eg., \(\mathit{\boldsymbol{y}}\) takes values of 0 and 1 denoting the two phenotype classes) are discrete values. Separation problem (complete separation and semi-complete separation) occurs often in such data, especially when epistasis is considered due to the larger number of variables \(p'=p(p+1)/2\). Of note, separation is a problem in logistic regression where there exist some coefficients \(\boldsymbol{\beta}\) such that  \(y_i=1\) whenever \(\mathbf{x}_i^T\boldsymbol{\beta} >0\), and \(y_i=0\) whenever \(\mathbf{x}_i^T\boldsymbol{\beta} < 0,  i = 1, \dots, n\). Unless the phenotype is a Mendelian trait, finding a genetic factor/set of factors that perfectly predict the phenotype outcome is too good to be true. While separation problem has been well documented in many statiscal textbook (eg., Ch9 of Altman M, Gill J, Mcdonald M P. (2005)), it is less studied in high dimensional sparse modeling methods. We next examine the problem using lasso as an example.</p>

<h4>Benchmarking using EBglmnet</h4>

<p>In the simulated F2 population, if we consider both main and epistatic effects, there will be a total number \(p' = 115,921\) candidate effects. We will randomly select 10 main and 10 interaction effects as true effects. we will use sample size \(n = 300\), and also randomly generate effect sizes from unif[2,3]. Note that an epistatic effect is generated by dot product of two interacting main effects:</p>

<pre><code class="r">n = 300;
set.seed(1)
index = sample(nrow(BASIS),n)
p = ncol(BASIS);
m = p*(p+1)/2;
#1. simulate true QTL locations
nMain = 10;
nEpis  = 10;
mainLoc = sample(p,nMain);
episLoc = sample(seq((p+1),m,1),nEpis);
trueLoc = sort(c(mainLoc,episLoc)); #a vector in [1,m]
nTrue = length(trueLoc);
trueLocs = ijIndex(trueLoc, p); #two columns denoting the pair (i,j)
#2. obtain true QTL genotype
basis = matrix(0,n,nTrue);
for(i in 1:nTrue)
{
    if(trueLocs[i,1]==trueLocs[i,2])
    {
        basis[,i] = BASIS[index,trueLocs[i,1]]
    }else
    {
        basis[,i] = BASIS[index,trueLocs[i,1]]*BASIS[index,trueLocs[i,2]]
    }
}
#3. simulate true QTL effect size   
trueEff  = runif(nTrue,2,3);
#4. simulate phenotype
xbeta = basis%*%trueEff;
vary = var(xbeta);
Pr = 1/(1+ exp( -xbeta));
y = rbinom(n,1,Pr);
</code></pre>

<p>Now we have the phenotype simulated. Let us demonstrate the data analysis using EBlasso-NE as an example. Given the significant larger number of candidate effects (200 times more effects), this will take a longer time for CV to finish  \((n_{folds} \times n_{hyperparameters} + 1)\) times of calling EBglmnet algorithm. In fact, the computational time is mostly determined by the number of nonzero effects selected by the model. This can be seen if a larger sample size is used (note: if you set \(n=1000\), this will take several more hours to finish the \((n_{folds} \times n_{hyperparameters} + 1)\) times computation (Altough a higher PD will be obtained).</p>

<pre><code class="r">CV = cv.EBglmnet(x=BASIS[index,],y=y,family=&quot;binomial&quot;,prior=&quot;lasso&quot;,nfold=5,Epis =TRUE)
ind = which(CV$fit[,6]&lt;=0.1)#p-value cutoff
CV$fit[ind,]
</code></pre>

<p>By comparing <code>trueLocs</code> and the effects identified by EBglmnet, it is demonstrated that EBlasso-NE has PD =0.50, FDR =0.09 using \(p\)-value cutoff = 0.10. As discussed earlier, lasso prior assigns a large probability mass in the two tails, resulting in a large number of small effects with large \(p\)-values. Without filtering using \(p-value\), EBlasso-NE has PD = 0.85 and FDR = 0.65.</p>

<h4>Separation Problem</h4>

<p>Compared with <code>EBglmnet</code>, <code>glmnet</code> does not have a reasonable result in analyzing this dataset, partially because of the separation problem. Let us first show the result using lasso approach.</p>

<p>Since glmnet has no built-in facility for epistasis analysis, we will manually create a genotype matrix \(\mathbf{X}\) containing all main and epistasis effects. </p>

<pre><code class="r">X = matrix(0,n,m);
X[,1:p] = BASIS[index,];
kk = p + 1;
for(i in 1:(p-1))
{
    for(j in (i+1):p)
    {
        X[,kk] = BASIS[index,i] * BASIS[index,j];
        kk = kk + 1;
    }
}
</code></pre>

<p>Let us analyze the dataset using lasso, examine the lasso selection path:</p>

<pre><code class="r">library(glmnet);
</code></pre>

<pre><code>## Loading required package: Matrix
## Loading required package: foreach
## Loaded glmnet 2.0-2
</code></pre>

<pre><code class="r">alpha = 1
lambdaRatio = 1e-4; #same as in EBlasso
cv = cv.glmnet(X, y, alpha = alpha,family=&quot;binomial&quot;,nfolds = 5,lambda.min.ratio=lambdaRatio)
nLambda = length(cv$lambda)
nLambda
</code></pre>

<pre><code>## [1] 78
</code></pre>

<pre><code class="r">nbeta = rep(0,nLambda);
fit0 = cv$glmnet.fit;
for(i in 1:nLambda)
{
  nbeta[i] = length(which(fit0$beta[,i]!=0))
}
plot(nbeta,xlab=expression(paste(lambda, &quot; in lasso selection path(n=300,p=115,921)&quot;)),
     ylab=&quot;No. of nonzero effects&quot;,xaxt=&quot;n&quot;)#
ticks = seq(1,nLambda,10)
axis(side=1, at= ticks,labels=round(cv$lambda[ticks],5), las=1,cex.axis = 0.5)
title(&quot;Number of nonzero effects in lasso selection path&quot;)
</code></pre>

<p><img src="data:image/png;base64,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" alt="plot of chunk unnamed-chunk-20"/> 
The result above demonstrated that lasso was not able to complete the selection path, and exited at the 78 out of 100 candidate $\lambda$s. From the scatterplot, it is shown that the number of nonzero effects has stablized after the 25th \(\lambda\) due to semi-complete separation that fewer candidate variables can be selected, even lambda is decreasing exponentially (will be explained in lasso discarding rule in the ensuing section). See the <code>glmnet</code> user manual for the built-in exit mechanism.</p>

<p>We can also take  a closer look by re-fitting the lasso selected effects in an ordinary logistic regression model, which will explicitly print out the warning message of separation:</p>

<pre><code class="r">lambda= cv$lambda.min
coefs = fit0$beta
ind = which(cv$lambda==cv$lambda.min)
beta = coefs[,ind]
betaij = which(beta!=0)
Xdata = X[,betaij];
colnames(Xdata) = betaij; 
refit = glm(y ~ Xdata, family = binomial(link = &quot;logit&quot;))#separation occurs
</code></pre>

<pre><code>## Warning: glm.fit: algorithm did not converge
</code></pre>

<pre><code>## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
</code></pre>

<p>The warning message describes that separation occurs. Separation problem is detrimental to lasso/elastic net due to the discarding rules (Tibshirani et al., 2012). Of note, the lasso discarding rule for logistic regression states that variable \(\boldsymbol{x}_j\) can be discarded if:</p>

<p>\[
|\mathit{\boldsymbol{x}}_j^T(\mathit{\boldsymbol{y}} - \boldsymbol{p}(\hat{\boldsymbol{\beta}}_{\lambda_{k-1}}))| < 2\lambda - \lambda_{k-1}, \forall j
\]</p>

<p>where \(\hat{\boldsymbol{\beta}}_{\lambda_{k-1}}\) is the nonzero coefficients found by lasso using \(\lambda_{k-1}\) at the \(k-1\) step. Suppose with \(\lambda_{k-1}\), lasso selected a set of variables \(\tilde{\boldsymbol{X}}\) that perfectly separate \(\mathit{\boldsymbol{y}}\), which lead to \(\mathit{\boldsymbol{y}} - \boldsymbol{p}(\hat{\boldsymbol{\beta}}_{\lambda_{k-1}}) \approx 0\). Then, the above discarding rule will have most of the remaining variables discarded. Note that \(\mathit{\boldsymbol{y}} - \boldsymbol{p}(\hat{\boldsymbol{\beta}}_{\lambda_{k-1}})\) will not be exactly 0 due to numerical estimations, and as \(\lambda\) decreases exponentially, a few variables can still pass the discarding rule (shown from 25th - 78th \(\lambda\) in the above example). However, finding a genetic factor/set of factors that perfectly predict the phenotype outcome is unlikely, and it is the case given the simulation setup. With the perfect separation provided by \(\tilde{\boldsymbol{X}}\), other true effects cannot be selected into the zon-zero set, resulting in limited PD.</p>

<p>EBglmnet doesn&#39;t implement such type of discarding rule and is more numerically stable. In the above simulation, EBglmnet can still identify several true effects with reasonable FDR. More simulation results using EBglmnet are available in (Huang et al., 2013) and the EBglmnet Application Note.</p>

<p><a id="parallel"></a></p>

<p>##Parallel Cross Validation</p>

<p>Although <code>EBglmnet</code> is efficient in inferring model parameters with given hyperparameters, there is no efficient selection path and discarding rules implemented.  Additionally, when epistasis is considered, <code>EBglmnet</code> allocates more computational resources to dynamically create the interaction variables so as to get around of memory shortage problem (by not creating the giant input matrix of \(n \times p'\) and other temporary variables). Evaluating \((n_{folds} \times n_{hyperparameters} + 1)\) times in serial can take many hours for a large dataset. Alternatively, in workstation/computer cluster environment, users can take advantage of parallel computational resources provided by R to reduce the computational time.  In the following example, the <code>snow</code> package will be used to demonstrate parallel CV.</p>

<pre><code class="r">#1. create the hyperparameters to be evaluated
familyPara = &quot;gaussian&quot;;
priorPara = &quot;elastic net&quot;;
epis =TRUE;
lambda_Max = lambdaMax(BASIS[index,],y,epis);
nStep = 9;#steps from lambda_max to 0.0001*lambda_min
lambda_Min = 0.0001*lambda_Max;
step = (log(lambda_Max) - log(lambda_Min))/nStep;
Lambda  = exp(seq(from = log(lambda_Max),to=log(lambda_Min),by= -step))
N_step  = length(Lambda);
Alpha = seq(from = 1, to = 0.1, by = -0.1)# values of alpha
nAlpha = length(Alpha); 
nPara = nAlpha * N_step;
HyperPara = matrix(0,nPara,2);
for(j in 1:nAlpha)
{
    strt = (j-1)*N_step + 1;
    ed  = (j-1)*N_step + N_step;
    HyperPara[strt:ed,1] = rep(Alpha[j],N_step);
    HyperPara[strt:ed,2] = Lambda;
}

#2. create a function for parallel computaiton
EBglmnet.CVonePar &lt;-function(iHyper,X,y,nFolds,foldId,hyperPara,...)
{
  parameters= hyperPara[iHyper,];
  SSE = CVonePair(X,y,nFolds,foldId,parameters,...);
  return(SSE);
}

#3. setup parallel computation
library(snow)
library(EBglmnet)
ncl = 4; #use 4 CPUs
cl&lt;-makeCluster(ncl,type=&quot;SOCK&quot;)
clusterEvalQ(cl,{library(EBglmnet)})
iPar = matrix(seq(1,nPara,1),nPara,1);
nFolds = 5;#5 fold CV
if(n%%nFolds!=0){
    foldId= sample(c(rep(1:nFolds,floor(n/nFolds)),1:(n%%nFolds)),n);
}else{
    foldId= sample(rep(1:nFolds,floor(n/nFolds)),n);
}
#call parRapply to perform parallel computation
SSE = parRapply(cl,iPar,EBglmnet.CVonePar,BASIS[index,],y,nFolds,foldId,HyperPara,
                Epis = epis, prior = priorPara, family= familyPara);    
#collect the result in CVresult
CVresult =matrix(SSE,nPara,4,byrow=TRUE)#4 columns of cv$
stopCluster(cl)
</code></pre>

<h2>References</h2>

<h3>EBglmnet Algorithms:</h3>

<p>Anhui Huang, Shizhong Xu, and Xiaodong Cai. (2015). <br>
<a href="http://www.nature.com/hdy/journal/v114/n1/full/hdy201479a.html/">Empirical Bayesian elastic net for multiple quantitative trait locus mapping.</a><br>
<em>Heredity</em>, Vol. 114(1), 107-115.</p>

<p>Anhui Huang, Shizhong Xu, and Xiaodong Cai. (2014a).<br>
<a href="http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0087330/">Whole-genome quantitative trait locus mapping reveals major role of epistasis on yield of rice.</a><br>
<em>PLoS ONE</em>, Vol. 9(1) e87330.</p>

<p>Anhui Huang, Eden Martin, Jeffery Vance, and Xiaodong Cai (2014b).<br>
<a href="http://onlinelibrary.wiley.com/doi/10.1002/gepi.21803/abstract?userIsAuthenticated=false&deniedAccessCustomisedMessage=/">Detecting genetic interactions in pathway-based genome-wide association studies.</a><br>
<em>Genetic Epidemiology</em>, 38(4), 300-309.</p>

<p> Anhui Huang, Shizhong Xu, and Xiaodong Cai. (2013). <br>
<a href="http://www.biomedcentral.com/1471-2156/14/5">Empirical Bayesian LASSO-logistic regression for multiple binary trait locus mapping. </a><br>
<em>BMC Genetics</em>, 14(1),5.</p>

<p>Xiaodong Cai, Anhui Huang, and Shizhong Xu (2011). <br>
<a href="http://www.biomedcentral.com/1471-2105/12/211">Fast empirical Bayesian LASSO for multiple quantitative trait locus mapping. </a><br>
<em>BMC Bioinformatics</em>, 12(1),211.</p>

<h2>Lasso and glmnet package</h2>

<p>Tibshirani, R., Bien, J., Friedman, J., Hastie, T., Simon, N., Taylor, J.,and Tibshirani, R.J., 2012. Strong rules for discarding predictors in lasso-type problems.  J. R. Stat. Soc. Series B. Stat. Methodol. 74, 245-266.</p>

<p>Friedman, J., T. Hastie, et al. (2010).  Regularization paths for generalized linear models via coordinate descent.  J. Stat. Softw. 33(1): 1-22.</p>

<p>Park, T. and G. Casella (2008). The Bayesian lasso. J. Am. Stat. Assoc. 103(482): 681-686.</p>

<p>Yi, N., and S. Xu (2008).  Bayesian LASSO for quantitative trait loci mapping. Genetics 179(2): 1045-1055.</p>

<p>Zou, H., and T. Hastie (2005). Regularization and variable selection via the elastic net. J. Roy. Stat. Soc. B. Met. 67(2): 301-320.</p>

<p>Tibshirani, R. (1996). Regression shrinkage and selection via the lasso. J. Roy. Stat. Soc. B. Met. 58(1): 267-288.</p>

<h2>Separation in logistic regression</h2>

<p>Altman M, Gill J, and Mcdonald M P. Numerical Issues in Statistical Computing for the Social Scientist. Journal of the American Statistical Association, 2005, 100(470):707-708.</p>

</body>

</html>
